é¡¹ç›® 'machinetranslation-2api' çš„ç»“æ„æ ‘:
ğŸ“‚ machinetranslation-2api/
    ğŸ“„ .env
    ğŸ“„ .env.example
    ğŸ“„ Dockerfile
    ğŸ“„ docker-compose.yml
    ğŸ“„ main.py
    ğŸ“„ nginx.conf
    ğŸ“„ requirements.txt
    ğŸ“‚ app/
        ğŸ“‚ core/
            ğŸ“„ __init__.py
            ğŸ“„ config.py
        ğŸ“‚ providers/
            ğŸ“„ __init__.py
            ğŸ“„ base_provider.py
            ğŸ“„ machinetranslation_provider.py
        ğŸ“‚ utils/
            ğŸ“„ sse_utils.py
================================================================================

--- æ–‡ä»¶è·¯å¾„: .env ---

# ====================================================================
# machinetranslation-2api é…ç½®æ–‡ä»¶æ¨¡æ¿
# ====================================================================
#
# è¯·å°†æ­¤æ–‡ä»¶é‡å‘½åä¸º ".env" å¹¶æŒ‰éœ€ä¿®æ”¹ã€‚
#

# --- æ ¸å¿ƒå®‰å…¨é…ç½® (å¿…é¡»è®¾ç½®) ---
# ç”¨äºä¿æŠ¤æ‚¨ API æœåŠ¡çš„è®¿é—®å¯†é’¥ã€‚
API_MASTER_KEY=1

# --- éƒ¨ç½²é…ç½® (å¯é€‰) ---
# Nginx å¯¹å¤–æš´éœ²çš„ç«¯å£
NGINX_PORT=8088


--- æ–‡ä»¶è·¯å¾„: .env.example ---

# ====================================================================
# machinetranslation-2api é…ç½®æ–‡ä»¶æ¨¡æ¿
# ====================================================================
#
# è¯·å°†æ­¤æ–‡ä»¶é‡å‘½åä¸º ".env" å¹¶æŒ‰éœ€ä¿®æ”¹ã€‚
#

# --- æ ¸å¿ƒå®‰å…¨é…ç½® (å¿…é¡»è®¾ç½®) ---
# ç”¨äºä¿æŠ¤æ‚¨ API æœåŠ¡çš„è®¿é—®å¯†é’¥ã€‚
API_MASTER_KEY=sk-machinetranslation-2api-default-key

# --- éƒ¨ç½²é…ç½® (å¯é€‰) ---
# Nginx å¯¹å¤–æš´éœ²çš„ç«¯å£
NGINX_PORT=8088


--- æ–‡ä»¶è·¯å¾„: Dockerfile ---

# ====================================================================
# Dockerfile for machinetranslation-2api (v1.0)
# ====================================================================

FROM python:3.10-slim

# è®¾ç½®ç¯å¢ƒå˜é‡
ENV PYTHONDONTWRITEBYTECODE=1
ENV PYTHONUNBUFFERED=1
WORKDIR /app

# å®‰è£… Python ä¾èµ–
COPY requirements.txt .
RUN pip install --no-cache-dir --upgrade pip && \
    pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY . .

# åˆ›å»ºå¹¶åˆ‡æ¢åˆ°é root ç”¨æˆ·
RUN useradd --create-home appuser && \
    chown -R appuser:appuser /app
USER appuser

# æš´éœ²ç«¯å£å¹¶å¯åŠ¨
EXPOSE 8000
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "1"]


--- æ–‡ä»¶è·¯å¾„: docker-compose.yml ---

services:
  nginx:
    image: nginx:latest
    container_name: machinetranslation-2api-nginx
    restart: always
    ports:
      - "${NGINX_PORT:-8088}:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
    depends_on:
      - app
    networks:
      - mt-net

  app:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: machinetranslation-2api-app
    restart: unless-stopped
    env_file:
      - .env
    networks:
      - mt-net

networks:
  mt-net:
    driver: bridge


--- æ–‡ä»¶è·¯å¾„: main.py ---

import sys
from contextlib import asynccontextmanager
from typing import Optional

from fastapi import FastAPI, Request, HTTPException, Depends, Header
from fastapi.responses import JSONResponse, StreamingResponse
from loguru import logger

from app.core.config import settings
from app.providers.machinetranslation_provider import MachineTranslationProvider

# --- ç»ˆææ—¥å¿—é…ç½® ---
logger.remove()
logger.add(
    sys.stdout,
    level="TRACE",  # å°†æ—¥å¿—çº§åˆ«è°ƒæ•´ä¸º TRACE ä»¥æŸ¥çœ‹æ‰€æœ‰ç»†èŠ‚
    format="<green>{time:YYYY-MM-DD HH:mm:ss.SSS}</green> | "
           "<level>{level: <8}</level> | "
           "<cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>",
    colorize=True,
    serialize=False
)

provider: Optional[MachineTranslationProvider] = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    global provider
    logger.info(f"åº”ç”¨å¯åŠ¨ä¸­... {settings.APP_NAME} v{settings.APP_VERSION}")
    provider = MachineTranslationProvider()
    logger.info(f"æœåŠ¡å°†åœ¨ http://localhost:{settings.NGINX_PORT} ä¸Šå¯ç”¨")
    yield
    logger.info("åº”ç”¨å…³é—­ã€‚")

app = FastAPI(
    title=settings.APP_NAME,
    version=settings.APP_VERSION,
    description=settings.DESCRIPTION,
    lifespan=lifespan
)

async def verify_api_key(authorization: Optional[str] = Header(None)):
    if settings.API_MASTER_KEY and settings.API_MASTER_KEY != "1":
        if not authorization or "bearer" not in authorization.lower():
            raise HTTPException(status_code=401, detail="éœ€è¦ Bearer Token è®¤è¯ã€‚")
        token = authorization.split(" ")[-1]
        if token != settings.API_MASTER_KEY:
            raise HTTPException(status_code=403, detail="æ— æ•ˆçš„ API Keyã€‚")

@app.post("/v1/chat/completions", dependencies=[Depends(verify_api_key)])
async def chat_completions(request: Request):
    """
    æ ¸å¿ƒèŠå¤©è¡¥å…¨ç«¯ç‚¹ã€‚
    ä¸ºäº†ç¡®ä¿ä¸æ‰€æœ‰å®¢æˆ·ç«¯ï¼ˆå¦‚ Cherry Studioï¼‰çš„æœ€ä½³å…¼å®¹æ€§ï¼Œ
    æ­¤ç«¯ç‚¹ç°åœ¨å§‹ç»ˆä»¥æµå¼ï¼ˆtext/event-streamï¼‰æ–¹å¼å“åº”ã€‚
    """
    request_data = await request.json()
    return StreamingResponse(
        provider.translate_stream(request_data), 
        media_type="text/event-stream"
    )

@app.get("/v1/models", dependencies=[Depends(verify_api_key)])
async def list_models():
    return await provider.get_models()

@app.get("/", summary="æ ¹è·¯å¾„")
def root():
    return {"message": f"æ¬¢è¿æ¥åˆ° {settings.APP_NAME} v{settings.APP_VERSION}. æœåŠ¡è¿è¡Œæ­£å¸¸ã€‚"}


--- æ–‡ä»¶è·¯å¾„: nginx.conf ---

worker_processes auto;

events {
    worker_connections 1024;
}

http {
    upstream mt_backend {
        server app:8000;
    }

    server {
        listen 80;
        server_name localhost;

        location / {
            proxy_pass http://mt_backend;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            
            # éæµå¼ APIï¼Œä½†ä¿ç•™æ˜¯è‰¯å¥½å®è·µ
            proxy_read_timeout 300s;
            proxy_connect_timeout 75s;
        }
    }
}


--- æ–‡ä»¶è·¯å¾„: requirements.txt ---

fastapi
uvicorn[standard]
pydantic-settings
python-dotenv
httpx
loguru


--- æ–‡ä»¶è·¯å¾„: app\core\__init__.py ---



--- æ–‡ä»¶è·¯å¾„: app\core\config.py ---

from pydantic_settings import BaseSettings, SettingsConfigDict
from typing import List, Optional

class Settings(BaseSettings):
    model_config = SettingsConfigDict(
        env_file=".env",
        env_file_encoding='utf-8',
        extra="ignore"
    )

    APP_NAME: str = "machinetranslation-2api"
    APP_VERSION: str = "1.0.0"
    DESCRIPTION: str = "ä¸€ä¸ªå°† machinetranslation.com è½¬æ¢ä¸ºå…¼å®¹ OpenAI æ ¼å¼ API çš„é«˜æ€§èƒ½ä»£ç†ã€‚"

    API_MASTER_KEY: Optional[str] = "sk-machinetranslation-2api-default-key"
    NGINX_PORT: int = 8088

    # ä¸Šæ¸¸ API é…ç½®
    # ä»æŠ“åŒ…ä¸­æå–çš„é™æ€ API Key
    MT_API_KEY: str = "pDwCjq7CyeAmn1Z3osNunACg2U0SLIhwBTtsp1WqYFMf5UuSIvMBYGS4pt8OIsGMH"
    BASE_API_URL: str = "https://api.machinetranslation.com/v1"
    SOCKET_URL: str = "https://ss.machinetranslation.com"
    
    API_REQUEST_TIMEOUT: int = 120
    SOCKET_TIMEOUT: int = 60 # ç­‰å¾… socket.io ç»“æœçš„è¶…æ—¶æ—¶é—´

    # æ¨¡å‹åˆ—è¡¨
    KNOWN_MODELS: List[str] = [
        "machinetranslation-best", "chat_gpt", "gemini", "claude", "libre", "mistral_ai", "smart"
    ]
    DEFAULT_MODEL: str = "machinetranslation-best"
    # ç”¨äºè·å–æœ€ç»ˆè¯„åˆ†æŠ¥å‘Šçš„æ¨¡å‹
    SCORER_MODEL: str = "gpt-4o-mini"
    # ç¿»è¯‘è¯·æ±‚ä¸­è¦åŒ…å«çš„æ¨¡å‹åˆ—è¡¨
    LLM_LIST_FOR_REQUEST: List[str] = ["chat_gpt", "gemini", "claude", "libre", "mistral_ai"]


settings = Settings()


--- æ–‡ä»¶è·¯å¾„: app\providers\__init__.py ---



--- æ–‡ä»¶è·¯å¾„: app\providers\base_provider.py ---

from abc import ABC, abstractmethod
from typing import Dict, Any
from fastapi.responses import StreamingResponse, JSONResponse

class BaseProvider(ABC):
    @abstractmethod
    async def chat_completion(
        self,
        request_data: Dict[str, Any]
    ) -> StreamingResponse:
        pass

    @abstractmethod
    async def get_models(self) -> JSONResponse:
        pass


--- æ–‡ä»¶è·¯å¾„: app\providers\machinetranslation_provider.py ---

import asyncio
import json
import time
import uuid
import re
from typing import Dict, Any, List, AsyncGenerator

import httpx
from fastapi import HTTPException
from loguru import logger

from app.core.config import settings
from app.utils.sse_utils import create_sse_data, create_chat_completion_chunk, DONE_CHUNK

# --- ç»ˆææ—¥å¿—æ¨¡å— ---
async def log_request(request):
    logger.debug(f">>> REQUEST: {request.method} {request.url}")
    logger.trace(f"    HEADERS: {request.headers}")
    try:
        content = request.content.decode('utf-8')
        if content: logger.trace(f"    BODY: {content}")
    except (UnicodeDecodeError, AttributeError):
        pass

async def log_response(response):
    await response.aread()
    logger.debug(f"<<< RESPONSE: {response.status_code} {response.request.method} {response.request.url}")
    logger.trace(f"    HEADERS: {response.headers}")
    try:
        content = response.text
        if content: logger.trace(f"    BODY: {content}")
    except Exception:
        pass
# --- ç»ˆææ—¥å¿—æ¨¡å—ç»“æŸ ---


class MachineTranslationProvider:
    def __init__(self):
        self.client = httpx.AsyncClient(
            timeout=settings.API_REQUEST_TIMEOUT,
            event_hooks={'request': [log_request], 'response': [log_response]}
        )
        self.base_headers = {
            "Accept": "*/*",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
            "Origin": "https://www.machinetranslation.com",
            "Referer": "https://www.machinetranslation.com/",
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/141.0.0.0 Safari/537.36",
        }
        self.api_headers = {**self.base_headers, "api-key": settings.MT_API_KEY}

    async def _get_share_id(self, text: str, source_lang: str, target_lang: str) -> str:
        url = f"{settings.BASE_API_URL}/translation/share-id"
        payload = {
            "text": text, "source_language_code": source_lang, "target_language_code": target_lang,
            "s3_file_path": None, "total_words": None, "secure_mode": False,
            "total_words_in_file": None, "is_doc_translation_disabled": False, "doc_translation_disabled_reason": ""
        }
        response = await self.client.post(url, headers=self.api_headers, json=payload)
        response.raise_for_status()
        data = response.json()
        share_id = data.get("share_id")
        if not share_id: raise ValueError("API å“åº”ä¸­ç¼ºå°‘ 'share_id'")
        logger.info(f"æˆåŠŸè·å– share_id: {share_id}")
        return share_id

    async def _manual_socket_io_flow(self, share_id: str) -> List[Dict]:
        sid = None
        try:
            t_param = f"{int(time.time() * 1000)}"
            handshake_url = f"{settings.SOCKET_URL}/socket.io/?EIO=4&transport=polling&t={t_param}"
            logger.info(f"[Socket-IO] æ­¥éª¤ 1: å‘é€æ¡æ‰‹è¯·æ±‚")
            response = await self.client.get(handshake_url, headers=self.base_headers)
            response.raise_for_status()
            
            raw_data = response.text
            match = re.search(r'"sid":"([^"]+)"', raw_data)
            if not match: raise ValueError("æ— æ³•ä»æ¡æ‰‹å“åº”ä¸­è§£æ sid")
            sid = match.group(1)
            logger.success(f"[Socket-IO] æˆåŠŸè·å– sid: {sid}")

            post_url = f"{settings.SOCKET_URL}/socket.io/?EIO=4&transport=polling&sid={sid}"
            
            connect_payload = f'40{json.dumps({"shareId": share_id}, separators=(",", ":"))}'
            logger.info("[Socket-IO] æ­¥éª¤ 2a: å‘é€å¸¦ shareId çš„ CONNECT (40) åŒ…")
            await self.client.post(post_url, headers={**self.base_headers, "Content-Type": "text/plain;charset=UTF-8"}, data=connect_payload.encode('utf-8'))
            
            event_payload = f'42["llm:translation:request",{{"shareId":"{share_id}","llmList":{json.dumps(settings.LLM_LIST_FOR_REQUEST)}}}]'
            logger.info(f"[Socket-IO] æ­¥éª¤ 2b: å‘é€ EVENT åŒ…")
            await self.client.post(post_url, headers={**self.base_headers, "Content-Type": "text/plain;charset=UTF-8"}, data=event_payload.encode('utf-8'))

            logger.info("[Socket-IO] æ­¥éª¤ 3: å¼€å§‹é•¿è½®è¯¢è·å–ç»“æœ...")
            results = []
            start_time = time.time()
            expected_results = len(settings.LLM_LIST_FOR_REQUEST)

            while time.time() - start_time < settings.SOCKET_TIMEOUT and len(results) < expected_results:
                t_param = f"{int(time.time() * 1000)}"
                poll_url = f"{settings.SOCKET_URL}/socket.io/?EIO=4&transport=polling&t={t_param}&sid={sid}"
                
                try:
                    poll_response = await self.client.get(poll_url, headers=self.base_headers, timeout=30)
                    poll_response.raise_for_status()
                    raw_poll_data = poll_response.text
                    
                    packets = raw_poll_data.split('')
                    for packet in packets:
                        if not packet: continue
                        if packet == '2':
                            logger.info("[Socket-IO] æ”¶åˆ° PING(2)ï¼Œç«‹å³å›å¤ PONG(3)")
                            await self.client.post(post_url, headers=self.base_headers, content='3')
                            continue
                        if packet.startswith('44'):
                            raise Exception(f"ä¸Šæ¸¸æ‹’ç»ä¼šè¯: {packet}")
                        if packet.startswith('42'):
                            logger.success(f"[Socket-IO] æ”¶åˆ°æ•°æ®åŒ…: {packet}")
                            try:
                                data = json.loads(packet[2:])
                                if data[0] == "llm:translation:response":
                                    results.append(data[1])
                                    logger.success(f"æˆåŠŸè§£æç¿»è¯‘ç»“æœ ({data[1].get('llm')})ã€‚è¿›åº¦: {len(results)}/{expected_results}")
                            except (json.JSONDecodeError, IndexError):
                                logger.warning(f"æ— æ³•è§£æäº‹ä»¶åŒ…: {packet}")
                except httpx.ReadTimeout:
                    logger.info("[Socket-IO] è½®è¯¢è¶…æ—¶ï¼Œç»§ç»­...")
                    continue
                except Exception as e:
                    logger.error(f"[Socket-IO] è½®è¯¢æœŸé—´å‘ç”Ÿé”™è¯¯: {e}")
                    break
            return results
        except Exception as e:
            logger.error(f"æ‰‹åŠ¨ Socket.IO æµç¨‹å¤±è´¥: {e}", exc_info=True)
            return []

    async def _get_final_scores(self, share_id: str) -> Dict:
        url = f"{settings.BASE_API_URL}/translation/score_test/{share_id}/{settings.SCORER_MODEL}"
        await asyncio.sleep(5)
        response = await self.client.post(url, headers=self.api_headers, content=b'', timeout=60)
        response.raise_for_status()
        data = response.json()
        logger.success(f"æˆåŠŸè·å– share_id '{share_id}' çš„æœ€ç»ˆè¯„åˆ†æŠ¥å‘Šã€‚")
        return data

    def _format_markdown_content(self, model: str, final_data: Dict) -> str:
        translations = final_data.get("translations", [])
        if not translations:
            return "é”™è¯¯ï¼šä¸Šæ¸¸æœªè¿”å›ä»»ä½•ç¿»è¯‘ç»“æœã€‚"

        best_translation = max(translations, key=lambda t: t.get("score") or 0)
        
        if model == "machinetranslation-best":
            main_content = best_translation.get("target_text", "No translation found.")
        else:
            specific_model_translation = next((t for t in translations if t.get("engine") == model), best_translation)
            main_content = specific_model_translation.get("target_text", "No translation found.")

        final_content = main_content.strip()
        final_content += "\n\n---\n\n### è¯¦ç»†ç¿»è¯‘æŠ¥å‘Š\n"
        
        sorted_translations = sorted(translations, key=lambda t: t.get("score") or 0, reverse=True)
        
        for t in sorted_translations:
            engine = t.get("engine", "N/A")
            score = t.get("score")
            score_str = f"{score:.2f}" if score is not None else "N/A"
            text = t.get("target_text", "").strip()
            
            final_content += f"\n**æ¨¡å‹: {engine}** (è¯„åˆ†: {score_str})\n"
            final_content += f"> {text}\n"
        
        return final_content

    async def translate_stream(self, request_data: Dict[str, Any]) -> AsyncGenerator[bytes, None]:
        request_id = f"chatcmpl-{uuid.uuid4()}"
        model = request_data.get("model", settings.DEFAULT_MODEL)
        
        try:
            messages = request_data.get("messages", [])
            text_to_translate = next((m['content'] for m in reversed(messages) if m.get('role') == 'user'), None)
            if not text_to_translate:
                raise HTTPException(status_code=400, detail="æœªæ‰¾åˆ°ç”¨æˆ·è¾“å…¥å†…å®¹ã€‚")

            share_id = await self._get_share_id(text_to_translate, "auto", "en")
            
            socket_results = await self._manual_socket_io_flow(share_id)
            if not socket_results:
                logger.warning("Socket æµç¨‹æœªè¿”å›ç»“æœï¼Œä½†ä»å°è¯•è·å–æœ€ç»ˆè¯„åˆ†æŠ¥å‘Š...")
            
            final_data = await self._get_final_scores(share_id)
            
            if not final_data.get("translations"):
                 raise HTTPException(status_code=502, detail="ä¸Šæ¸¸æœªè¿”å›ä»»ä½•ç¿»è¯‘ç»“æœã€‚")

            markdown_content = self._format_markdown_content(model, final_data)

            # ä»¥æµçš„å½¢å¼ä¸€æ¬¡æ€§å‘é€æ‰€æœ‰å†…å®¹
            chunk = create_chat_completion_chunk(request_id, model, markdown_content)
            yield create_sse_data(chunk)

            # å‘é€ç»“æŸæ ‡å¿—
            final_chunk = create_chat_completion_chunk(request_id, model, "", finish_reason="stop")
            yield create_sse_data(final_chunk)

        except Exception as e:
            logger.error(f"æµå¼å¤„ç†ä¸­å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
            error_message = f"å¤„ç†è¯·æ±‚æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
            chunk = create_chat_completion_chunk(request_id, model, error_message, finish_reason="error")
            yield create_sse_data(chunk)
        
        finally:
            yield DONE_CHUNK

    async def get_models(self) -> Dict:
        return {
            "object": "list",
            "data": [
                {"id": name, "object": "model", "created": int(time.time()), "owned_by": "lzA6"}
                for name in settings.KNOWN_MODELS
            ]
        }


--- æ–‡ä»¶è·¯å¾„: app\utils\sse_utils.py ---

import json
import time
from typing import Dict, Any, Optional

DONE_CHUNK = b"data: [DONE]\n\n"

def create_sse_data(data: Dict[str, Any]) -> bytes:
    """å°†å­—å…¸æ•°æ®æ ¼å¼åŒ–ä¸º SSE äº‹ä»¶å­—ç¬¦ä¸²ã€‚"""
    return f"data: {json.dumps(data, ensure_ascii=False)}\n\n".encode('utf-8')

def create_chat_completion_chunk(
    request_id: str,
    model: str,
    content: str,
    finish_reason: Optional[str] = None
) -> Dict[str, Any]:
    """åˆ›å»ºä¸€ä¸ªä¸ OpenAI å…¼å®¹çš„èŠå¤©è¡¥å…¨æµå¼å—ã€‚"""
    return {
        "id": request_id,
        "object": "chat.completion.chunk",
        "created": int(time.time()),
        "model": model,
        "choices": [
            {
                "index": 0,
                "delta": {"content": content},
                "finish_reason": finish_reason
            }
        ]
    }



